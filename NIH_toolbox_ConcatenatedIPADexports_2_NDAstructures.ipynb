{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Jupyter Notebook is written to convert Data and Scores files from NIH Toolbox IPAD exports into NDA data structures using linking information from a 'crosswalk' and extra NDA-required subject data from a csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Notes: \n",
    "using a specialty Python 3 virtual environment (named PycharmToolbox) as kernel for this notebook.\n",
    "Installed by running the following commands in my terminal and then switching the kernel with the dropdown menu above:\n",
    "> source /home/petra/.virtualenvs/PycharmToolbox/bin/activate\n",
    "> pip install ipykernel\n",
    "> ipython kernel install --user --name=PycharmToolbox\n",
    "> jupyter-notebook\n",
    "\n",
    "requirements file generated from within the activated virtual environment by:\n",
    "> pip freeze > requirements.txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "snapshotdate = datetime.datetime.today().strftime('%m_%d_%Y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the input and output data and paths for NIH toolbox.\n",
    "To run the cells of this notebook, you will need four files.\n",
    "\n",
    "Two are in the .csv format of the IPAD Toolbox applcation export.\n",
    "E.g. a raw Data file containing item level responses, and a Scores file, containing the summary statistics for the collection of item level data. We don't need the registration file.  These two files are linked by PIN and Inst variables, and must be cleaned a priori, to remove subjects that are in one but not the other file.  I.e. the list of unique PINs (ex. HCP0211999_V1) in one file should be exactly the same as the list of unique PINs in the other. For HCP data, we concatenate the exports of all subjects' Score data in to a single file, and the exports of all subjects Raw data into a second file.  Because all other sources of HCP data use 'subject' and 'visit' rather than a PIN which is a concatenation of both, we create these variables (subject and visit) from PIN prior to running this program as well.  \n",
    "\n",
    "The third necessary file is a csv containing the fields that NDA requires in all of their structures \n",
    "e.g. subjectkey (GUID or pseudo-GUID), src_subject_id (e.g. HCP0211999), interview_age (in months), and gender (misnomer for sex assigned at birth).  In HCP data, we link the two sources of information via 'subject' and 'visit.'  \n",
    "\n",
    "Lastly, read in the crosswalk file - which will map your vars to NDA after transpose is complete.  I have placed the crosswalk from HCPA as an example.  Any instruments in this crosswalk that are the same as yours (look at 'Inst' column) will work for you.  You will have to add any instruments not present, after obtaining variable maps and templates from the NDA for your particular set of NIH Toolbox Data.  \n",
    "\n",
    "Note that subject and visit are variables we created locally to merge with the data coming from a different local source (REDCap).  They are not variables that are output from the NIH Toolbox app on the Ipads, but are necessary for the merge with the NDA required fields stored elsewhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for formatted structures\n",
    "pathout=\"/home/petra/UbWinSharedSpace1/ccf-nda-behavioral/PycharmToolbox/Ipad2NDA_withCrosswalk/NIHToolbox2NDA/prepped_structures/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv scores and raw files for transformation - \n",
    "scoresD='/home/petra/UbWinSharedSpace1/boxtemp/HCAorBoth_Toolbox_Scored_Combined_12_17_2019.csv'\n",
    "rawD='/home/petra/UbWinSharedSpace1/boxtemp/HCAorBoth_Toolbox_Raw_Combined_12_17_2019.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age-Corrected Standard Score</th>\n",
       "      <th>Age-Corrected Standard Scores Dominant</th>\n",
       "      <th>Age-Corrected Standard Scores Non-Dominant</th>\n",
       "      <th>AgeCorrCrystal</th>\n",
       "      <th>AgeCorrDCCS</th>\n",
       "      <th>AgeCorrEarly</th>\n",
       "      <th>AgeCorrEngRead</th>\n",
       "      <th>AgeCorrEngVocab</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>iPad Version</th>\n",
       "      <th>pin</th>\n",
       "      <th>raw_cat_date</th>\n",
       "      <th>site</th>\n",
       "      <th>source</th>\n",
       "      <th>study</th>\n",
       "      <th>subject</th>\n",
       "      <th>v1_interview_date</th>\n",
       "      <th>visit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iPad Air 2 (WiFi)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12_12_2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCA6058970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iPad Air 2 (WiFi)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12_12_2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCA6058970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iPad Air 2 (WiFi)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12_12_2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCA6058970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iPad Air 2 (WiFi)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12_12_2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCA6058970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iPad Air 2 (WiFi)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12_12_2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCA6058970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Age  Age-Corrected Standard Score  \\\n",
       "0           0  NaN                         101.0   \n",
       "1           1  NaN                          97.0   \n",
       "2           2  NaN                          97.0   \n",
       "3           3  NaN                         120.0   \n",
       "4           4  NaN                          78.0   \n",
       "\n",
       "   Age-Corrected Standard Scores Dominant  \\\n",
       "0                                     NaN   \n",
       "1                                     NaN   \n",
       "2                                     NaN   \n",
       "3                                     NaN   \n",
       "4                                     NaN   \n",
       "\n",
       "   Age-Corrected Standard Scores Non-Dominant  AgeCorrCrystal  AgeCorrDCCS  \\\n",
       "0                                         NaN             NaN          NaN   \n",
       "1                                         NaN             NaN          NaN   \n",
       "2                                         NaN             NaN          NaN   \n",
       "3                                         NaN             NaN          NaN   \n",
       "4                                         NaN             NaN          NaN   \n",
       "\n",
       "   AgeCorrEarly  AgeCorrEngRead  AgeCorrEngVocab  ...  gender  \\\n",
       "0           NaN             NaN              NaN  ...     NaN   \n",
       "1           NaN             NaN              NaN  ...     NaN   \n",
       "2           NaN             NaN              NaN  ...     NaN   \n",
       "3           NaN             NaN              NaN  ...     NaN   \n",
       "4           NaN             NaN              NaN  ...     NaN   \n",
       "\n",
       "        iPad Version  pin  raw_cat_date  site  source study     subject  \\\n",
       "0  iPad Air 2 (WiFi)  NaN    12_12_2019   NaN     NaN   NaN  HCA6058970   \n",
       "1  iPad Air 2 (WiFi)  NaN    12_12_2019   NaN     NaN   NaN  HCA6058970   \n",
       "2  iPad Air 2 (WiFi)  NaN    12_12_2019   NaN     NaN   NaN  HCA6058970   \n",
       "3  iPad Air 2 (WiFi)  NaN    12_12_2019   NaN     NaN   NaN  HCA6058970   \n",
       "4  iPad Air 2 (WiFi)  NaN    12_12_2019   NaN     NaN   NaN  HCA6058970   \n",
       "\n",
       "   v1_interview_date  visit  \n",
       "0                NaN     V1  \n",
       "1                NaN     V1  \n",
       "2                NaN     V1  \n",
       "3                NaN     V1  \n",
       "4                NaN     V1  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read into dataframe and take a peak\n",
    "scordata=pd.read_csv(scoresD,header=0,low_memory=False)\n",
    "scordata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>App Version</th>\n",
       "      <th>Assessment Name</th>\n",
       "      <th>DataType</th>\n",
       "      <th>DateCreated</th>\n",
       "      <th>DateCreatedDatetime</th>\n",
       "      <th>DeviceID</th>\n",
       "      <th>Education</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>...</th>\n",
       "      <th>index</th>\n",
       "      <th>level_0</th>\n",
       "      <th>parent</th>\n",
       "      <th>raw_cat_date</th>\n",
       "      <th>site</th>\n",
       "      <th>source</th>\n",
       "      <th>study</th>\n",
       "      <th>subject</th>\n",
       "      <th>v1_interview_date</th>\n",
       "      <th>visit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19.2160</td>\n",
       "      <td>Assessment 1</td>\n",
       "      <td>informational</td>\n",
       "      <td>1/29/19 11:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6D3999FF-2614-43C3-BB7E-82729622914B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12_12_2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCA6058970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19.2160</td>\n",
       "      <td>Assessment 1</td>\n",
       "      <td>informational</td>\n",
       "      <td>1/29/19 11:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6D3999FF-2614-43C3-BB7E-82729622914B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12_12_2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCA6058970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19.2160</td>\n",
       "      <td>Assessment 1</td>\n",
       "      <td>integer</td>\n",
       "      <td>1/29/19 11:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6D3999FF-2614-43C3-BB7E-82729622914B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12_12_2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCA6058970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19.2160</td>\n",
       "      <td>Assessment 1</td>\n",
       "      <td>integer</td>\n",
       "      <td>1/29/19 11:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6D3999FF-2614-43C3-BB7E-82729622914B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12_12_2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCA6058970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19.2160</td>\n",
       "      <td>Assessment 1</td>\n",
       "      <td>informational</td>\n",
       "      <td>1/29/19 11:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6D3999FF-2614-43C3-BB7E-82729622914B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12_12_2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCA6058970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Age App Version Assessment Name       DataType    DateCreated  \\\n",
       "0           0  NaN   1.19.2160    Assessment 1  informational  1/29/19 11:37   \n",
       "1           1  NaN   1.19.2160    Assessment 1  informational  1/29/19 11:37   \n",
       "2           2  NaN   1.19.2160    Assessment 1        integer  1/29/19 11:37   \n",
       "3           3  NaN   1.19.2160    Assessment 1        integer  1/29/19 11:38   \n",
       "4           4  NaN   1.19.2160    Assessment 1  informational  1/29/19 11:38   \n",
       "\n",
       "  DateCreatedDatetime                              DeviceID  Education  \\\n",
       "0                 NaN  6D3999FF-2614-43C3-BB7E-82729622914B        NaN   \n",
       "1                 NaN  6D3999FF-2614-43C3-BB7E-82729622914B        NaN   \n",
       "2                 NaN  6D3999FF-2614-43C3-BB7E-82729622914B        NaN   \n",
       "3                 NaN  6D3999FF-2614-43C3-BB7E-82729622914B        NaN   \n",
       "4                 NaN  6D3999FF-2614-43C3-BB7E-82729622914B        NaN   \n",
       "\n",
       "   Ethnicity  ...  index level_0 parent  raw_cat_date  site  source study  \\\n",
       "0        NaN  ...    NaN     NaN    NaN    12_12_2019   NaN     NaN   NaN   \n",
       "1        NaN  ...    NaN     NaN    NaN    12_12_2019   NaN     NaN   NaN   \n",
       "2        NaN  ...    NaN     NaN    NaN    12_12_2019   NaN     NaN   NaN   \n",
       "3        NaN  ...    NaN     NaN    NaN    12_12_2019   NaN     NaN   NaN   \n",
       "4        NaN  ...    NaN     NaN    NaN    12_12_2019   NaN     NaN   NaN   \n",
       "\n",
       "      subject v1_interview_date  visit  \n",
       "0  HCA6058970               NaN     V1  \n",
       "1  HCA6058970               NaN     V1  \n",
       "2  HCA6058970               NaN     V1  \n",
       "3  HCA6058970               NaN     V1  \n",
       "4  HCA6058970               NaN     V1  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata=pd.read_csv(rawD,header=0,low_memory=False)\n",
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Age', 'App Version', 'Assessment Name', 'DataType',\n",
       "       'DateCreated', 'DateCreatedDatetime', 'DeviceID', 'Education',\n",
       "       'Ethnicity', 'FathersEducation', 'Firmware Version', 'FirstDate4PIN',\n",
       "       'Gender', 'GuardiansEducation', 'Handedness', 'Inst', 'InstEnded',\n",
       "       'InstEndedDatetime', 'InstOrdr', 'InstSctn', 'InstStarted',\n",
       "       'InstStartedDatetime', 'ItemID', 'ItmOrdr', 'Locale',\n",
       "       'MothersEducation', 'Name', 'PIN', 'Position', 'Race', 'Response',\n",
       "       'ResponseTime', 'SE', 'Score', 'StartingLevelOverride', 'TScore',\n",
       "       'Theta', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'datediff', 'file_id',\n",
       "       'filename', 'flagged', 'gender', 'iPad Version', 'index', 'level_0',\n",
       "       'parent', 'raw_cat_date', 'site', 'source', 'study', 'subject',\n",
       "       'v1_interview_date', 'visit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.columns #dont use gender from rawdata ... use gender from NDA specialty fields - ROSETTA STONE ALERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep the fields that NDA requires in all of their structures - we did this in another program, since output is required elsewhere\n",
    "#here, just subsetting ROSETTA STONE to particular study, renaming a few vars, and changing the date format\n",
    "subjectlist='/home/petra/UbWinSharedSpace1/redcap2nda_Lifespan2019/Dev_pedigrees/UnrelatedHCAHCD_w_STG_Image_and_pseudo_GUID09_27_2019.csv'\n",
    "subjects=pd.read_csv(subjectlist)[['subjectped','nda_gender', 'nda_guid', 'nda_interview_age', 'nda_interview_date']]\n",
    "ndar=subjects.loc[subjects.subjectped.str.contains('HCA')].rename(\n",
    "    columns={'nda_guid':'subjectkey','subjectped':'src_subject_id','nda_interview_age':'interview_age',\n",
    "             'nda_interview_date':'interview_date','nda_gender':'gender'}).copy()\n",
    "ndar['interview_date'] = pd.to_datetime(ndar['interview_date']).dt.strftime('%m/%d/%Y')\n",
    "ndarlist=['subjectkey','src_subject_id','interview_age','interview_date','gender']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the list of variables in the scored and raw data files that you might need...\n",
    "#creating list in case your scored data is merged with other files for other reasons (ours was)\n",
    "scorlist=['Age-Corrected Standard Score', 'Age-Corrected Standard Scores Dominant',\n",
    " 'Age-Corrected Standard Scores Non-Dominant', 'AgeCorrCrystal', 'AgeCorrDCCS', 'AgeCorrEarly',\n",
    " 'AgeCorrEngRead', 'AgeCorrEngVocab', 'AgeCorrFlanker', 'AgeCorrFluid', 'AgeCorrListSort',\n",
    " 'AgeCorrPSM', 'AgeCorrPatternComp', 'AgeCorrTotal', 'Assessment Name', 'Computed Score',\n",
    " 'ComputedDCCS', 'ComputedEngRead', 'ComputedEngVocab', 'ComputedFlanker', 'ComputedPSM',\n",
    " 'ComputedPatternComp', 'DCCSaccuracy', 'DCCSreactiontime',  'Dominant Score', 'FlankerAccuracy',\n",
    " 'FlankerReactionTime', 'FullTCrystal', 'FullTDCCS', 'FullTEarly', 'FullTEngRead', 'FullTEngVocab',\n",
    " 'FullTFlanker', 'FullTFluid', 'FullTListSort', 'FullTPSM', 'FullTPatternComp', 'FullTTotal',\n",
    " 'Fully-Corrected T-score', 'Fully-Corrected T-scores Dominant', 'Fully-Corrected T-scores Non-Dominant',\n",
    " 'FullyCorrectedTscore', 'Group', 'Inst', 'InstrumentBreakoff', 'InstrumentRCReason', 'InstrumentRCReasonOther',\n",
    " 'InstrumentStatus2', 'ItmCnt', 'Language', 'Male', 'National Percentile (age adjusted)',\n",
    " 'National Percentile (age adjusted) Dominant', 'National Percentile (age adjusted) Non-Dominant',\n",
    " 'Non-Dominant Score', 'PIN', 'Raw Score Left Ear', 'Raw Score Right Ear', 'RawDCCS',\n",
    " 'RawFlanker', 'RawListSort', 'RawPSM', 'RawPatternComp', 'RawScore', 'SE', 'Static Visual Acuity Snellen',\n",
    " 'Static Visual Acuity logMAR', 'TScore', 'Theta', 'ThetaEngRead', 'ThetaEngVocab', 'ThetaPSM', 'Threshold Left Ear',\n",
    " 'Threshold Right Ear', 'UncorrCrystal', 'UncorrDCCS', 'UncorrEarly', 'UncorrEngRead', 'UncorrEngVocab',\n",
    " 'UncorrFlanker', 'UncorrFluid', 'UncorrListSort', 'UncorrPSM', 'UncorrPatternComp', 'UncorrTotal',\n",
    " 'Uncorrected Standard Score', 'Uncorrected Standard Scores Dominant', 'Uncorrected Standard Scores Non-Dominant',\n",
    " 'UncorrectedStandardScore']\n",
    "rawlist=['App Version', 'Assessment Name', 'DataType','DateCreated', 'DeviceID',  'Firmware Version',  \n",
    " 'Inst', 'InstEnded','InstEndedDatetime', 'InstOrdr', 'InstSctn', 'InstStarted','InstStartedDatetime',\n",
    " 'ItemID', 'ItmOrdr', 'Locale','PIN', 'Position', 'Response', 'ResponseTime', 'SE', 'Score', 'TScore',\n",
    " 'Theta','iPad Version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the score and raw data with the required fields for the NDA\n",
    "#Note that subject and visit are HCP specific variables that we use to subset the records being sent to the NDA\n",
    "#create dummy vars if you dont have them...\n",
    "#scordata['subject']=scordata.PIN #or some other variable in scordata that can be used to merge with ndarfields data\n",
    "#scordata['visit']='V1' #we keep this around because eventually we'll be releaseing V2,V3, and FU data\n",
    "#rawdata['subject']=rawdata.PIN\n",
    "#rawdata['visit']='V1'\n",
    "\n",
    "scordata=pd.merge(scordata[scorlist+['subject','visit']],ndar,how='inner',left_on='subject', right_on='src_subject_id')\n",
    "rawdata=pd.merge(rawdata[rawlist+['subject','visit']],ndar,how='inner',left_on='subject', right_on='src_subject_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot index with vector containing NA / NaN values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-f3a1cead8643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#scordata.loc[scordata.Inst.str.contains('Fluid')].Inst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrawdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fluid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/PycharmToolbox/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/PycharmToolbox/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1866\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1868\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1869\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/PycharmToolbox/lib/python3.6/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot index with vector containing NA / NaN values"
     ]
    }
   ],
   "source": [
    "#scordata.loc[scordata.Inst.str.contains('Fluid')].Inst\n",
    "\n",
    "rawdata.loc[rawdata.Inst.str.contains('Fluid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inst</th>\n",
       "      <th>template</th>\n",
       "      <th>inst_short</th>\n",
       "      <th>Source</th>\n",
       "      <th>nda_structure</th>\n",
       "      <th>nda_element</th>\n",
       "      <th>hcp_variable</th>\n",
       "      <th>action_requested</th>\n",
       "      <th>hcp_variable_upload</th>\n",
       "      <th>specialty_code</th>\n",
       "      <th>requested_python</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety Summary Parent Report (3-7)</td>\n",
       "      <td>Anxiety_Summary_3-7.tlbx_fearanx01_template</td>\n",
       "      <td>Anxiety_Summary_3-7</td>\n",
       "      <td>HCPD</td>\n",
       "      <td>tlbx_fearanx01</td>\n",
       "      <td>version_form</td>\n",
       "      <td>Assessment_Name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessment_Name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anxiety Summary Parent Report (3-7)</td>\n",
       "      <td>Anxiety_Summary_3-7.tlbx_fearanx01_template</td>\n",
       "      <td>Anxiety_Summary_3-7</td>\n",
       "      <td>HCPD</td>\n",
       "      <td>tlbx_fearanx01</td>\n",
       "      <td>nih_tlbx_fctsc</td>\n",
       "      <td>Fully_Corrected_T_score</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fully_Corrected_T_score</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anxiety Summary Parent Report (3-7)</td>\n",
       "      <td>Anxiety_Summary_3-7.tlbx_fearanx01_template</td>\n",
       "      <td>Anxiety_Summary_3-7</td>\n",
       "      <td>HCPD</td>\n",
       "      <td>tlbx_fearanx01</td>\n",
       "      <td>primary_language</td>\n",
       "      <td>Language</td>\n",
       "      <td>Please rename 'Language' to 'primary_language'...</td>\n",
       "      <td>primary_language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>studydata['primary_language']=studydata['Langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cognition Composite Scores</td>\n",
       "      <td>cogcomp01_template</td>\n",
       "      <td>cogcomp01</td>\n",
       "      <td>HCPD HCPA</td>\n",
       "      <td>cogcomp01</td>\n",
       "      <td>version_form</td>\n",
       "      <td>Assessment_Name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessment_Name</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cognition Composite Scores</td>\n",
       "      <td>cogcomp01_template</td>\n",
       "      <td>cogcomp01</td>\n",
       "      <td>HCPD HCPA</td>\n",
       "      <td>cogcomp01</td>\n",
       "      <td>interview_language</td>\n",
       "      <td>Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Language</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Inst  \\\n",
       "0  Anxiety Summary Parent Report (3-7)   \n",
       "1  Anxiety Summary Parent Report (3-7)   \n",
       "2  Anxiety Summary Parent Report (3-7)   \n",
       "3           Cognition Composite Scores   \n",
       "4           Cognition Composite Scores   \n",
       "\n",
       "                                      template           inst_short  \\\n",
       "0  Anxiety_Summary_3-7.tlbx_fearanx01_template  Anxiety_Summary_3-7   \n",
       "1  Anxiety_Summary_3-7.tlbx_fearanx01_template  Anxiety_Summary_3-7   \n",
       "2  Anxiety_Summary_3-7.tlbx_fearanx01_template  Anxiety_Summary_3-7   \n",
       "3                           cogcomp01_template            cogcomp01   \n",
       "4                           cogcomp01_template            cogcomp01   \n",
       "\n",
       "      Source   nda_structure         nda_element             hcp_variable  \\\n",
       "0       HCPD  tlbx_fearanx01        version_form          Assessment_Name   \n",
       "1       HCPD  tlbx_fearanx01      nih_tlbx_fctsc  Fully_Corrected_T_score   \n",
       "2       HCPD  tlbx_fearanx01    primary_language                 Language   \n",
       "3  HCPD HCPA       cogcomp01        version_form          Assessment_Name   \n",
       "4  HCPD HCPA       cogcomp01  interview_language                 Language   \n",
       "\n",
       "                                    action_requested      hcp_variable_upload  \\\n",
       "0                                                NaN          Assessment_Name   \n",
       "1                                                NaN  Fully_Corrected_T_score   \n",
       "2  Please rename 'Language' to 'primary_language'...         primary_language   \n",
       "3                                                NaN          Assessment_Name   \n",
       "4                                                NaN                 Language   \n",
       "\n",
       "  specialty_code                                   requested_python  \n",
       "0            NaN                                                NaN  \n",
       "1            NaN                                                NaN  \n",
       "2            NaN  studydata['primary_language']=studydata['Langu...  \n",
       "3              1                                                NaN  \n",
       "4              1                                                NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify your crosswalk- take a peak - use the latest crosswalk from the https://github.com/humanconnectome/NIHToolbox2NDA/\n",
    "#e.g. NIH_Toolbox_crosswalk_HCP.csv\n",
    "crosswalkfile=\"/home/petra/UbWinSharedSpace1/ccf-nda-behavioral/PycharmToolbox/Ipad2NDA_withCrosswalk/NIHToolbox2NDA/NIH_Toolbox_crosswalk_HCP.csv\"\n",
    "crosswalk=pd.read_csv(crosswalkfile,header=0,low_memory=False, encoding = \"ISO-8859-1\")\n",
    "crosswalk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a little QC and data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruments in Raw data but not Scores:\n",
      "NIH Toolbox Pattern Comparison Processing Speed Test Age 7+ Practice v2.1\n",
      "NIH Toolbox Pain Intensity FF Age 18+ v2.0\n",
      "nan\n",
      "Instruments in Scored data but not Raw:\n",
      "Cognition Fluid Composite v1.1\n",
      "Cognition Crystallized Composite v1.1\n",
      "Cognition Total Composite Score v1.1\n",
      "Cognition Early Childhood Composite v1.1\n",
      "NIH Toolbox Visual Acuity Practice Age 8+ v2.0\n",
      "Negative Affect Summary (18+)\n",
      "Social Satisfaction Summary (18+)\n",
      "Psychological Well Being Summary (18+)\n",
      "NIH Toolbox Emotion Instructions (Adult/Child) v1.0\n"
     ]
    }
   ],
   "source": [
    "#check that your instruments are in both raw data and scores files. \n",
    "#For HCP, all but the NIH Toolbox Pain Intensity FF Age 18+ v2.0 Instrument are practices\n",
    "#So only the Pain Intensity instrument needed special coding attention (to be dealt with later)\n",
    "#check your data and adjust if needed \n",
    "print('Instruments in Raw data but not Scores:')\n",
    "for i in rawdata.Inst.unique():\n",
    "    if i not in scordata.Inst.unique():\n",
    "        print(i)\n",
    "print('Instruments in Scored data but not Raw:')\n",
    "for i in scordata.Inst.unique():\n",
    "    if i not in rawdata.Inst.unique():\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669\n",
      "669\n",
      "(235573, 32)\n",
      "(25264, 96)\n",
      "(235573, 32)\n",
      "(25264, 96)\n"
     ]
    }
   ],
   "source": [
    "#check that lengths are the same...indicating one to one PIN match between scores and raw\n",
    "print(len(rawdata.PIN.unique()))\n",
    "print(len(scordata.PIN.unique()))\n",
    "#check that shape is same before and after removing duplicates (should not be any)\n",
    "rawdata.shape\n",
    "scordata.shape\n",
    "print(rawdata.shape)\n",
    "print(scordata.shape)\n",
    "testraw=rawdata.drop_duplicates(subset={'PIN','Inst','ItemID','Position'},keep='first')\n",
    "testscore=scordata.drop_duplicates(subset={'PIN','Inst'})\n",
    "print(testraw.shape)\n",
    "print(testscore.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function that will turn a dataframe into a csv structure \n",
    "#- use the definition to send the pain data (which doesn't have entries in the scored data)\n",
    "def data2struct(patho,dout,crosssub,study='HCPD'):\n",
    "    \"\"\"\n",
    "    Convert dout, a prepared pandas dataframe, into a csv structure that NDA can import\n",
    "    \n",
    "    parameters: \n",
    "    patho - full path to place you want to store structures (there will be many)\n",
    "    dout - name of data frame that contains all the variables to be exported\n",
    "    crosssub - a dataframe which is the subset of the crosswalk for the instrument to be exported as structure\n",
    "    study - a string to put in the name of the csv file along with the structure name and the short name of the instrument\n",
    "    \n",
    "    note that snapshotdate is defined external to this funtion near import statments...     \n",
    "    \n",
    "    \"\"\"\n",
    "    strucroot=crosssub['nda_structure'].str.strip().str[:-2][0]\n",
    "    strucnum=crosssub['nda_structure'].str.strip().str[-2:][0]\n",
    "    instshort=crosssub['inst_short'].str.strip()[0]\n",
    "    filePath=os.path.join(pathout,study+'_'+instshort+'_'+strucroot+strucnum+'_'+snapshotdate+'.csv')\n",
    "    if os.path.exists(filePath):\n",
    "        os.remove(filePath)\n",
    "    else:\n",
    "        pass\n",
    "        #print(\"Can not delete the file as it doesn't exists\")\n",
    "    with open(filePath,'a') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        dout.to_csv(f,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function that can be used for the instruments that follow more generalizable pattern\n",
    "#This function will alert you to any instruments that were successfully transformed but tha might\n",
    "#warrent a closer look.\n",
    "def sendthroughcrosswalk(pathout,instreshapedfull,inst_i,crosswalk,studystr='HCPD'):\n",
    "    # replace special charaters in column names\n",
    "    instreshapedfull.columns = instreshapedfull.columns.str.replace(' ', '_').str.replace('-', '_').str.replace('(','_').str.replace(')', '_')\n",
    "    crosswalk_subset = crosswalk.loc[crosswalk['Inst'] == inst_i]\n",
    "    crosswalk_subset.reset_index(inplace=True)\n",
    "    # crosswalk_subset.loc[crosswalk_subset['hcp_variable_upload'].isnull()==False,'hcp_variable']\n",
    "    cwlistbef = list(crosswalk_subset['hcp_variable'])\n",
    "    before = len(cwlistbef)\n",
    "    cwlist = list(set(cwlistbef) & set(\n",
    "        instreshapedfull.columns))  # drop the handful of vars in larger instruments that got mapped but that we dont have\n",
    "    after = len(cwlist)\n",
    "    if before != after:\n",
    "        print(\"WARNING!!! \" + inst_i + \": Crosswalk expects \" + str(before) + \" elements, but only found \" + str(after))\n",
    "        notfound=list(np.setdiff1d(cwlistbef,cwlist))\n",
    "        print(\"Not Found:\"+ str(notfound))\n",
    "    studydata = instreshapedfull[ndarlist + cwlist].copy()\n",
    "    # execute any python one liners\n",
    "    for index, row in crosswalk_subset.iterrows():\n",
    "        if pd.isna(row['requested_python']):\n",
    "            pass\n",
    "        else:\n",
    "            exec(row['requested_python'])\n",
    "    uploadlist = list(crosswalk_subset['hcp_variable_upload'])\n",
    "    uploadlist = list(set(uploadlist) & set(studydata.columns))\n",
    "    data2struct(patho=pathout, dout=studydata[ndarlist + uploadlist], crosssub=crosswalk_subset, study=studystr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do special cases first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Within the rawdata structure (for HCP), all but the NIH Toolbox Pain Intensity FF Age 18+ v2.0 Instrument are practices\n",
    "#So only the Pain Intensity instrument needed special coding attention\n",
    "#check your data and adjust if needed - note that subject and visit are variables we created locally \n",
    "#to merge with the data coming from a different local source (REDCap)\n",
    "#create the NDA structure for this special case\n",
    "inst_i='NIH Toolbox Pain Intensity FF Age 18+ v2.0'\n",
    "#most of the rows contain duplicated information...only need to know the PIN once, for example, not once for each item response\n",
    "# so values in the response column need to be pivoted and then merged with the rest of the data, \n",
    "paindata=rawdata.loc[rawdata.Inst==inst_i][['PIN','subject','Inst','visit','ItemID','Position',\n",
    "        'subjectkey','src_subject_id','interview_age','interview_date','gender',\n",
    "        'Response','ResponseTime', 'SE', 'Score', 'TScore','Theta']]\n",
    "paindata.ItemID = paindata.ItemID.str.lower().str.replace('-','_').str.replace('(','_').str.replace(')','_')\n",
    "inst = paindata.pivot(index='PIN', columns='ItemID', values='Response').reset_index()\n",
    "meta = paindata.drop_duplicates(subset=['PIN', 'visit'])\n",
    "painreshaped = pd.merge(meta, inst, on='PIN', how='inner').drop(columns={'subject','visit','PIN'})\n",
    "crosswalk_subset=crosswalk.loc[crosswalk['Inst']==inst_i]\n",
    "crosswalk_subset.reset_index(inplace=True)\n",
    "cwlist=list(crosswalk_subset['hcp_variable_upload'])\n",
    "\n",
    "#several dummy vars for required vars\n",
    "painreshaped['nih_tlbx_agegencsc']=999\n",
    "painreshaped['nih_tlbx_rawscore']=999\n",
    "painreshaped['nih_tlbx_tscore']=999\n",
    "painreshaped['nih_tlbx_se']=999\n",
    "painreshaped['nih_tlbx_theta']=999\n",
    "\n",
    "reshapedslim=painreshaped[ndarlist+cwlist]\n",
    "data2struct(patho=pathout,dout=reshapedslim,crosssub=crosswalk_subset,study='HCPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another special case is for Cognition Composite scores - going to cogcomp01 structure at the NDA- \n",
    "# This was mapped before Leo agreed to accept data by NIH Toolbox Instrument name (pivot by Inst)\n",
    "# keeping this special case coding in for posterity and to shed light on one type of merge he must do on his end, \n",
    "# when it comes to NIH toolbox data\n",
    "cogcompdata=scordata.loc[scordata.Inst.str.contains('Cognition')==True][['PIN','Language',\n",
    "    'Assessment Name','Inst',  'Uncorrected Standard Score', 'Age-Corrected Standard Score',\n",
    "    'National Percentile (age adjusted)', 'Fully-Corrected T-score']+ndarlist]\n",
    "\n",
    "#initialize prefix\n",
    "cogcompdata['varprefix']='test'\n",
    "cogcompdata.loc[cogcompdata.Inst=='Cognition Crystallized Composite v1.1','varprefix']='nih_crystalcogcomp_'\n",
    "cogcompdata.loc[cogcompdata.Inst=='Cognition Early Childhood Composite v1.1','varprefix']='nih_eccogcomp_'\n",
    "cogcompdata.loc[cogcompdata.Inst=='Cognition Fluid Composite v1.1','varprefix']='nih_fluidcogcomp_'\n",
    "cogcompdata.loc[cogcompdata.Inst=='Cognition Total Composite Score v1.1','varprefix']='nih_totalcogcomp_'\n",
    "#pivot the vars of interest by varprefix and rename\n",
    "uncorr=cogcompdata.pivot(index='PIN',columns='varprefix',values='Uncorrected Standard Score')\n",
    "for col in uncorr.columns.values:\n",
    "    uncorr=uncorr.rename(columns={col:col+\"unadjusted\"})\n",
    "ageadj=cogcompdata.pivot(index='PIN',columns='varprefix',values='Age-Corrected Standard Score')\n",
    "for col in ageadj.columns.values:\n",
    "    ageadj=ageadj.rename(columns={col:col+\"ageadj\"})\n",
    "npage=cogcompdata.pivot(index='PIN',columns='varprefix',values='National Percentile (age adjusted)')\n",
    "for col in npage.columns.values:\n",
    "    npage=npage.rename(columns={col:col+\"np_ageadj\"})\n",
    "#put them together\n",
    "cogcompreshape=pd.concat([uncorr,ageadj,npage],axis=1)\n",
    "meta=cogcompdata[['PIN','Language','Assessment Name']+ndarlist].drop_duplicates(subset={'PIN'})\n",
    "#these variables were intended to capture the version...they got mapped to raw scores, though, which isnt right.\n",
    "#ultimately decided to leave these out for now, but know they exist in the IPAD data but not the \n",
    "#NDA which wasnt thinking of different versions of subcomponents when they built this structure\n",
    "meta['nih_crystalcogcomp']='Cognition Crystallized Composite v1.1'\n",
    "meta['nih_eccogcomp']='Cognition Early Childhood Composite v1.1'\n",
    "meta['nih_fluidcogcomp']='Cognition Fluid Composite v1.1'\n",
    "meta['nih_totalcogcomp']='Cognition Total Composite Score v1.1'\n",
    "cogcompreshape=pd.merge(meta,cogcompreshape,on='PIN',how='inner')\n",
    "\n",
    "inst_i='Cognition Composite Scores'\n",
    "sendthroughcrosswalk(pathout,cogcompreshape,inst_i,crosswalk,studystr='HCPA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PIN', 'Language', 'Assessment_Name', 'subjectkey', 'src_subject_id',\n",
       "       'interview_age', 'interview_date', 'gender', 'nih_crystalcogcomp',\n",
       "       'nih_eccogcomp', 'nih_fluidcogcomp', 'nih_totalcogcomp',\n",
       "       'nih_crystalcogcomp_unadjusted', 'nih_eccogcomp_unadjusted',\n",
       "       'nih_fluidcogcomp_unadjusted', 'nih_totalcogcomp_unadjusted',\n",
       "       'nih_crystalcogcomp_ageadj', 'nih_eccogcomp_ageadj',\n",
       "       'nih_fluidcogcomp_ageadj', 'nih_totalcogcomp_ageadj',\n",
       "       'nih_crystalcogcomp_np_ageadj', 'nih_eccogcomp_np_ageadj',\n",
       "       'nih_fluidcogcomp_np_ageadj', 'nih_totalcogcomp_np_ageadj'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cogcompreshape.head()\n",
    "#cogcompdata.columns\n",
    "cogcompreshape.columns\n",
    "#cogcompreshape.nih_crystalcogcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last special Case is for Visual Acuity, which needs double pivot because of repeat items at different positions\n",
    "#This special case not yet mapped by NDA - so don't run, but will look something like this\n",
    "#special case for instruments with \"Visual Acuity\" in their titles, which have dup inst/itemid at diff positions\n",
    "#for i in scordata.Inst.unique():\n",
    "#    if i in rawdata.Inst.unique():\n",
    "#        inst_i=i\n",
    "#        if \"Visual Acuity\" in inst_i:\n",
    "#            print('Processing ' + inst_i + '...')\n",
    "#                items=rawdata.loc[rawdata.Inst.str.contains('Visual Acuity')][['PIN','subject','Inst',\n",
    "#                   'gender','visit','ItemID','Position','Response']]\n",
    "#                items.ItemID = items.ItemID.str.lower()\n",
    "#                items['dup_number']=items.groupby(['PIN','ItemID']).cumcount()+1\n",
    "#                items['ItemID_Dup']=items.ItemID.str.replace('|', '_') + '_P'+items.dup_number.astype(str)\n",
    "#                inst=items.pivot(index='PIN',columns='ItemID_Dup',values='Response')\n",
    "#                meta = items.drop_duplicates(subset=['PIN', 'visit'])[['Inst', 'PIN', \n",
    "#                                                               'subject', 'visit']]\n",
    "#                instreshaped = pd.merge(meta, inst, on='PIN', how='inner')\n",
    "#                items2 = scordata.loc[scordata.Inst == inst_i]\n",
    "#                instreshapedfull = pd.merge(instreshaped, items2, on='PIN', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!!! NIH Toolbox Picture Vocabulary Test Age 3+ v2.1: Crosswalk expects 290 elements, but only found 218\n",
      "Not Found:['lavoc048', 'lavoc068', 'lavoc081', 'lavoc084', 'lavoc091', 'lavoc096', 'lavoc100', 'lavoc103', 'lavoc112', 'lavoc118', 'lavoc119', 'lavoc120', 'lavoc126', 'lavoc127', 'lavoc128', 'lavoc132', 'lavoc134', 'lavoc136', 'lavoc137', 'lavoc143', 'lavoc145', 'lavoc148', 'lavoc152', 'lavoc155', 'lavoc157', 'lavoc158', 'lavoc160', 'lavoc161', 'lavoc165', 'lavoc169', 'lavoc176', 'lavoc177', 'lavoc181', 'lavoc182', 'lavoc183', 'lavoc184', 'lavoc187', 'lavoc188', 'lavoc189', 'lavoc190', 'lavoc191', 'lavoc193', 'lavoc194', 'lavoc197', 'lavoc199', 'lavoc203', 'lavoc205', 'lavoc206', 'lavoc214', 'lavoc215', 'lavoc216', 'lavoc224', 'lavoc225', 'lavoc229', 'lavoc231', 'lavoc233', 'lavoc236', 'lavoc237', 'lavoc240', 'lavoc242', 'lavoc259', 'lavoc303', 'lavoc306', 'lavoc315', 'lavoc317', 'lavoc329', 'lavoc337', 'lavoc346', 'lavoc364', 'lavoc422', 'lavoc452', 'lavoc473']\n",
      "WARNING!!! NIH Toolbox List Sorting Working Memory Test Age 7+ v2.1: Crosswalk expects 47 elements, but only found 35\n",
      "Not Found:[]\n",
      "WARNING!!! NIH Toolbox Oral Reading Recognition Test Age 3+ v2.1: Crosswalk expects 275 elements, but only found 180\n",
      "Not Found:['lare019', 'lare020', 'lare021', 'lare022', 'lare023', 'lare026', 'lare027', 'lare028', 'lare030', 'lare032', 'lare033', 'lare035', 'lare036', 'lare037', 'lare038', 'lare039', 'lare040', 'lare043', 'lare044', 'lare045', 'lare048', 'lare049', 'lare052', 'lare053', 'lare054', 'lare056', 'lare057', 'lare058', 'lare059', 'lare061', 'lare063', 'lare065', 'lare066', 'lare067', 'lare068', 'lare069', 'lare073', 'lare074', 'lare075', 'lare077', 'lare079', 'lare080', 'lare081', 'lare083', 'lare084', 'lare086', 'lare087', 'lare088', 'lare092', 'lare098', 'lare099', 'lare104', 'lare105', 'lare108', 'lare109', 'lare113', 'lare126', 'lare138', 'lare139', 'lare143', 'lare152', 'larebr2', 'larebr4', 'larebr5', 'larebr7', 'laresc001']\n",
      "Couldnt process NIH Toolbox Grip Strength Test Age 3+ v2.0...\n",
      "WARNING!!! NIH Toolbox Positive Affect CAT Age 18+ v2.0: Crosswalk expects 26 elements, but only found 25\n",
      "Not Found:['pa044']\n",
      "WARNING!!! NIH Toolbox Fear-Affect CAT Age 18+ v2.0: Crosswalk expects 36 elements, but only found 28\n",
      "Not Found:['anxiety41', 'anxiety43', 'anxiety46', 'anxiety48', 'anxiety50', 'anxiety53', 'anxiety56', 'anxiety64']\n",
      "WARNING!!! NIH Toolbox Sadness CAT Age 18+ v2.0: Crosswalk expects 35 elements, but only found 25\n",
      "Not Found:['depression31', 'depression33', 'depression34', 'depression35', 'depression37', 'depression42', 'depression51', 'depression52', 'depression53', 'depression55']\n",
      "WARNING!!! NIH Toolbox Anger-Affect CAT Age 18+ v2.0: Crosswalk expects 30 elements, but only found 27\n",
      "Not Found:['anger44', 'anger52']\n",
      "WARNING!!! NIH Toolbox Pain Interference CAT Age 18+ v2.0: Crosswalk expects 25 elements, but only found 22\n",
      "Not Found:['dummy6', 'dummy7', 'dummy8']\n",
      "WARNING!!! NIH Toolbox Words-In-Noise Test Age 6+ v2.1: Crosswalk expects 37 elements, but only found 31\n",
      "Not Found:['Raw Score Left Ear', 'Raw Score Right Ear', 'Threshold Left Ear', 'Threshold Right Ear', 'Words_In_Noise_Test__Dominant_Score', 'Words_In_Noise_Test__Non_Dominant_Score']\n",
      "WARNING!!! NIH Toolbox Picture Vocabulary Test Age 3+ v2.0: Crosswalk expects 250 elements, but only found 239\n",
      "Not Found:['lavoc091', 'lavoc120', 'lavoc188', 'lavoc199', 'lavoc225', 'lavoc237', 'lavoc241', 'lavoc242', 'lavoc259', 'lavoc330', 'lavoc452']\n",
      "WARNING!!! NIH Toolbox Oral Reading Recognition Test Age 3+ v2.0: Crosswalk expects 228 elements, but only found 211\n",
      "Not Found:['lare037', 'lare038', 'lare041', 'lare050', 'lare051', 'lare052', 'lare053', 'lare055', 'lare057', 'lare064', 'lare065', 'lare066', 'lare077', 'lare078', 'lare079', 'lare088', 'laresc001']\n",
      "WARNING!!! NIH Toolbox Dimensional Change Card Sort Test Ages 8-11 v2.1: Crosswalk expects 50 elements, but only found 46\n",
      "Not Found:['dccs_color_prac5', 'dccs_color_prac6', 'dccs_color_prac7', 'dccs_color_prac8']\n",
      "Couldnt process NIH Toolbox Friendship FF Ages 8-17 v2.0...\n",
      "WARNING!!! NIH Toolbox Self-Efficacy CAT Ages 8-12 v2.0: Crosswalk expects 18 elements, but only found 16\n",
      "Not Found:['gse02m', 'gse09']\n",
      "WARNING!!! NIH Toolbox Fear FF Ages 8-17 v2.0: Crosswalk expects 17 elements, but only found 16\n",
      "Not Found:['dummy_var1']\n",
      "WARNING!!! NIH Toolbox Sadness FF Ages 8-17 v2.0: Crosswalk expects 32 elements, but only found 16\n",
      "Not Found:[]\n",
      "Note:  Omitting practice instrument, NIH Toolbox Picture Vocabulary Test Age 3+ Practice v2.1\n",
      "Note:  Omitting practice instrument, NIH Toolbox Picture Vocabulary Test Age 3+ Practice v2.0\n"
     ]
    }
   ],
   "source": [
    "#for non-special instruments in both scores and raw data types\n",
    "for i in scordata.Inst.unique():\n",
    "    if i in rawdata.Inst.unique():\n",
    "        inst_i=i\n",
    "        if \"Visual Acuity\" in inst_i:\n",
    "            pass  #special case--see below\n",
    "        elif \"Practice\" in inst_i:\n",
    "            print(\"Note:  Omitting practice instrument, \"+inst_i)\n",
    "        else:\n",
    "            try:  #this will fail if there are duplicates or if no-one has the data of interest (e.g. idlist too small), or if only V2 instrument\n",
    "                #print('Processing '+inst_i+'...')\n",
    "                items=rawdata.loc[rawdata.Inst==inst_i][['PIN','subject','Inst','visit','ItemID','Position',\n",
    "                   'subjectkey','src_subject_id','interview_age','interview_date','gender',\n",
    "                   'Response','ResponseTime']]# not these..., 'SE', 'Score', 'TScore','Theta']]\n",
    "                items.ItemID = items.ItemID.str.lower().str.replace('-','_').str.replace('(','_').str.replace(')','_').str.replace(' ','_')\n",
    "                inst=items.pivot(index='PIN',columns='ItemID',values='Response').reset_index()\n",
    "                meta=items.drop_duplicates(subset=['PIN','visit'])\n",
    "                instreshaped = pd.merge(meta, inst, on='PIN', how='inner').drop(columns={'subject', 'visit','Inst'})\n",
    "                items2=scordata.loc[scordata.Inst==inst_i][scorlist]\n",
    "                instreshapedfull=pd.merge(instreshaped,items2,on='PIN',how='inner')\n",
    "                sendthroughcrosswalk(pathout,instreshapedfull, inst_i, crosswalk,studystr='HCPA')\n",
    "            except:\n",
    "                print('Couldnt process '+inst_i+'...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now validate all these files by calling the OS from within this notebook (assuming you are using linux) to run the NDA validator on your command line.  Alternatively, you could just close this notebook now and run the the following for loop below.  \n",
    "\n",
    "for var in pathout/*.csv; do vtcmd $var; done\n",
    "(for var in prepped_structures/*.csv; do vtcmd $var; done)\n",
    "\n",
    "Either option requires that you have downloaded and installed https://github.com/NDAR/nda-tools python package\n",
    "per instructions.  I installed vtcmd in my home directory, which set a couple defaults in place., such as the location of validation results. To have the output of the validation sent to a more meaningful location than than the default, open the /home/petra/.NDATools/settings.cfg file (wherever it resides in your system), and  \n",
    "change the line under [Files] that says 'validation_results = NDAValidationResults' to a better place (perhaps 'pathout').  Example, mine now says \n",
    "validation_results = /home/petra/UbWinSharedSpace1/ccf-nda-behavioral/PycharmToolbox/Ipad2NDA_withCrosswalk/NIHToolbox2NDA/NDAValidationResults\n",
    "\n",
    "so that the prepped structures directory and the NDAValidationResults Directory are right next to one another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6d68feacd375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#run your command and capture the log (which will probably report bug) for one file ot see how it works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vtcmd'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpathout\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"HCPA_Grip_Strength_tlbx_motor01_03_11_2020.csv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#example process for one structure \n",
    "#run your command and capture the log (which will probably report bug) for one file ot see how it works\n",
    "out=subprocess.Popen(['vtcmd',pathout+\"HCPA_Grip_Strength_tlbx_motor01_03_11_2020.csv\"], stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "stdout,stderr=out.communicate()\n",
    "print(stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had an error in the validation, your likely course of action is to add some extra python code to the crosswalk.   \n",
    "\n",
    "grep notInteger /home/petra/NDAValidationResults/* > Notintegerwarnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PycharmToolbox",
   "language": "python",
   "name": "pycharmtoolbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
